{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4480da3",
   "metadata": {},
   "source": [
    "![image](../images/kdd24-logo-small.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cfc0",
   "metadata": {},
   "source": [
    "## Hands-on Tutorial\n",
    "## Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices\n",
    "### Lab 2.2 : LLM Fine-Tuning through QLoRA.    \n",
    "#### Summary: \n",
    "This lab focused on Instruction fine-tuning a Meta-Llama-3-8B-Instruct model using CUAD data \n",
    "\n",
    "- The training dataset is from CUAD - BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF  \n",
    "- The training is transformers trainer through QLoRA     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abbdf5",
   "metadata": {},
   "source": [
    "### Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install sentence_transformers\n",
    "\n",
    "#!pip install continuous_eval\n",
    "#!pip install tiktoken\n",
    "\n",
    "#!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc2e29-ffd9-47df-971f-06b56f5aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df_train_data = pd.read_csv(TRN_FILE)\n",
    "\n",
    "sample = Dataset.from_pandas(df_train_data)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df473c1",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa94308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{INPUT_KEY}{sample['question']}\\n\" if sample['question'] else None\n",
    "    response = f\"{RESPONSE_KEY}{sample['answer']}\\n\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\".join(parts)\n",
    "\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "sample_p = create_prompt_formats(sample[randrange(9)])\n",
    "print(sample_p['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len((sample_p['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "\n",
    "    conf = model.config\n",
    "\n",
    "    max_length = None\n",
    "\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "            \n",
    "    if not max_length:\n",
    "        max_length = 8192\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "        \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['index', 'question', 'input', 'answer', 'qa_id', 'text'],\n",
    "    )\n",
    "\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f8c1f",
   "metadata": {},
   "source": [
    "### Setup model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd3d1c",
   "metadata": {},
   "source": [
    "Configuration model quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_4bit = True\n",
    "bnb_4bit_use_double_quant = True\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e82962",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de561378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f03ec",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31225a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\", \n",
    "    token = TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          token = TOKEN,\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217308b",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539719cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "seed = 0\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036de3d",
   "metadata": {},
   "source": [
    "### Setup training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ff5a9",
   "metadata": {},
   "source": [
    "Setup LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967db6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 64,    \n",
    "    lora_alpha = 256,    \n",
    "    target_modules = ['gate_proj', 'up_proj', 'q_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e73b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b693de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()    #reduce memory usage during fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc33097",
   "metadata": {},
   "source": [
    "Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\"\n",
    "per_device_train_batch_size = 1\n",
    "learning_rate = 1e-5   \n",
    "warmup_steps = 2  # Linear warmup steps from 0 to learning_rate \n",
    "fp16 = True  \n",
    "epoch = 2             \n",
    "logging_steps = epoch*2\n",
    "save_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = preprocessed_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        warmup_steps = warmup_steps,\n",
    "        num_train_epochs = epoch,\n",
    "        learning_rate = learning_rate,\n",
    "        fp16 = fp16,\n",
    "        logging_steps = logging_steps,\n",
    "        output_dir = output_dir,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps = save_steps,\n",
    "    ),\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1257dc",
   "metadata": {},
   "source": [
    "### Launch training and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "do_train = True\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "if do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving last checkpoint of the model...\")\n",
    "trainer.model.save_pretrained(output_dir,\n",
    "                              token = TOKEN,\n",
    "                              trust_remote_code=True,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547eac32",
   "metadata": {},
   "source": [
    "### Test inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8311fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "tokenizer_ft = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f44fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    #print(\"generated_text = \", response)\n",
    "    full_text = response.split('[Response]:')[1].split('[End]')[0].strip()\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 7\n",
    "\n",
    "query = df_train_data['question'][IDX]\n",
    "gt = df_train_data['answer'][IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a736a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb = f\"{INTRO_BLURB}\"\n",
    "instruction = f\"{INSTRUCTION_KEY}\"\n",
    "input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"\\nAnswer = \", answer, \"\\nGT = \", gt, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddbfb690",
   "metadata": {},
   "source": [
    "### Optional: restart the kernel and run batch inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- restart kernel ---\n",
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574ba88",
   "metadata": {},
   "source": [
    "Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2353df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921c581",
   "metadata": {},
   "source": [
    "Load FM and Peft-load adapter then merge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6047456",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_4bit = True\n",
    "bnb_4bit_use_double_quant = True\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69133618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d13ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = PeftModel.from_pretrained(\n",
    "    model_ft, \n",
    "    output_dir, \n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d016c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da8461",
   "metadata": {},
   "source": [
    "Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
    "tokenizer_ft.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c17c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127b607",
   "metadata": {},
   "source": [
    "Prepare for model inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09767c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    #print(\"generated_text = \", response)\n",
    "    if('[Response]:' in response):\n",
    "        full_text = response.split('[Response]:')[1].strip()\n",
    "        if ('[End]' in response):\n",
    "            full_text = full_text.split('[End]')[0].strip()\n",
    "    else:\n",
    "        full_text = response\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30430e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "boto3_bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "boto3_bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "def get_titan_embedding(text):\n",
    "    \n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    modelId = 'amazon.titan-embed-text-v2:0'     \n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'    \n",
    "    \n",
    "    response = boto3_bedrock_runtime.invoke_model(body=body, \n",
    "                                                  modelId=modelId, \n",
    "                                                  accept=accept, \n",
    "                                                  contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    \n",
    "    return embedding\n",
    "    \n",
    "def calculate_semantic_sim_titan(pred_list,ref_list):\n",
    "   \n",
    "    sem_score_titan = []\n",
    "    average_sem_sim = 0\n",
    "    \n",
    "    for i in range(len(ref_list)):\n",
    "        print(i,end = '|')\n",
    "        ref_embedding = get_titan_embedding(ref_list[i])\n",
    "        pred_embedding = get_titan_embedding(pred_list[i])\n",
    "        cos_sim = util.cos_sim(ref_embedding, pred_embedding)\n",
    "        \n",
    "        sem_score_titan.append(cos_sim[0][0].item())\n",
    "    \n",
    "    return sem_score_titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2efb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_eval.metrics.generation.text import DeterministicAnswerCorrectness\n",
    "\n",
    "def calculate_answer_correctness(pred_list,ref_list):\n",
    "   \n",
    "    token_overlap_recall = []\n",
    "    rouge_l_recall = []\n",
    "    \n",
    "    metric = DeterministicAnswerCorrectness()\n",
    "    \n",
    "    for i in range(len(ref_list)):\n",
    "        print(i,end = '|')\n",
    "    \n",
    "        datum = {\n",
    "            \"answer\": pred_list[i],\n",
    "            \"ground_truth_answers\": [ref_list[i]],\n",
    "        } \n",
    "        ac = metric(**datum)    \n",
    "        \n",
    "        token_overlap_recall.append(ac['token_overlap_recall'])\n",
    "        rouge_l_recall.append(ac['rouge_l_recall'])\n",
    "        \n",
    "    return token_overlap_recall, rouge_l_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbba1d",
   "metadata": {},
   "source": [
    "Test single inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df_test_data = pd.read_csv(TRN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 10\n",
    "\n",
    "query = df_test_data['question'][IDX]\n",
    "gt = df_test_data['answer'][IDX]\n",
    "\n",
    "blurb = f\"{INTRO_BLURB}\"\n",
    "instruction = f\"{INSTRUCTION_KEY}\"\n",
    "input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "\n",
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"\\nAnswer = \", answer, \"\\nGT = \", gt, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4dffd",
   "metadata": {},
   "source": [
    "Batch inference (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df_test_data = pd.read_csv(TRN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17080c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question_list = []\n",
    "test_answer_list = []\n",
    "test_ref_answer_list = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for i in range(len(df_test_data['question'])):\n",
    "    print(i,end='|')\n",
    "    \n",
    "    query = df_test_data['question'][i].strip()\n",
    "    ref_answer = df_test_data['answer'][i].strip()\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "    prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "\n",
    "    response_text,response_time = Llama_Infer(prompt)\n",
    "    print(response_text)\n",
    "    \n",
    "    test_question_list.append(query)\n",
    "    test_answer_list.append(response_text)\n",
    "    test_ref_answer_list.append(ref_answer)\n",
    "    \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ss_list = calculate_semantic_sim_titan(test_answer_list,test_ref_answer_list)\n",
    "test_tor_list, test_rlr_list_list = calculate_answer_correctness(test_answer_list,test_ref_answer_list)\n",
    "\n",
    "average_sem_sim_titan = np.average(test_ss_list)   \n",
    "average_sem_sim_titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f444a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = pd.DataFrame()  \n",
    "\n",
    "df_response[\"question\"] = test_question_list\n",
    "df_response[\"ref_answer\"] = test_ref_answer_list\n",
    "df_response[\"response\"] = test_answer_list\n",
    "df_response[\"semantic_similarity\"] = test_ss_list\n",
    "df_response[\"token_overlap_recall\"] = test_tor_list\n",
    "df_response[\"rouge_l_recall\"] = test_rlr_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abe7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_OUTPUT_FILE = '../lab-data/sft_trn_q4b_result.csv'\n",
    "df_response.to_csv(TEST_OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8b33c",
   "metadata": {},
   "source": [
    "Batch inference (testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE = '../lab-data/ENERGOUSCORP_qa_test.csv'\n",
    "df_test_data = pd.read_csv(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba76c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question_list = []\n",
    "test_answer_list = []\n",
    "test_ref_answer_list = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for i in range(len(df_test_data['question'])):\n",
    "    print(i,end='|')\n",
    "    \n",
    "    query = df_test_data['question'][i].strip()\n",
    "    ref_answer = df_test_data['answer'][i].strip()\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "    prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "\n",
    "    response_text,response_time = Llama_Infer(prompt)\n",
    "    print(response_text)\n",
    "    \n",
    "    test_question_list.append(query)\n",
    "    test_answer_list.append(response_text)\n",
    "    test_ref_answer_list.append(ref_answer)\n",
    "    \n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ss_list = calculate_semantic_sim_titan(test_answer_list,test_ref_answer_list)\n",
    "test_tor_list, test_rlr_list_list = calculate_answer_correctness(test_answer_list,test_ref_answer_list)\n",
    "\n",
    "average_sem_sim_titan = np.average(test_ss_list)   \n",
    "average_sem_sim_titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = pd.DataFrame()  \n",
    "\n",
    "df_response[\"question\"] = test_question_list\n",
    "df_response[\"ref_answer\"] = test_ref_answer_list\n",
    "df_response[\"response\"] = test_answer_list\n",
    "df_response[\"semantic_similarity\"] = test_ss_list\n",
    "df_response[\"token_overlap_recall\"] = test_tor_list\n",
    "df_response[\"rouge_l_recall\"] = test_rlr_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_OUTPUT_FILE = '../lab-data/sft_test_q4b_result.csv'\n",
    "df_response.to_csv(TEST_OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4676d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cfa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae073a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb823fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
