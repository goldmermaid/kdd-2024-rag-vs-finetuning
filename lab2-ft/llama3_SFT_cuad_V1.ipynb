{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e0e3e2",
   "metadata": {},
   "source": [
    "## PEFT/LoRA fine tuning with  dataset\n",
    "\n",
    "Instruction fine-tuning: meta.llama3-8b-instruct-v1:0 (meta-llama/Meta-Llama-3-8B-Instruct) with LoRA, no quantization\n",
    "     \n",
    "Training: transformer trainer   \n",
    "Data: CUAD - BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\n",
    "\n",
    "2024/07/31: first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7a204d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.43.3)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (1.26.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: peft in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.44.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.4.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.24.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install peft\n",
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349471e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061bce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 22:21:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   51C    P0              41W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31eebe",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "#from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a2028cb-361c-4c91-a3fa-34285224d83c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64de6-aa77-469a-bd9d-66c47f3fac8b",
   "metadata": {},
   "source": [
    "### Prepare data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10fc2e29-ffd9-47df-971f-06b56f5aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b34280b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACT_FILES = [\"BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\",\n",
    "                  \"ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT.PDF\",\n",
    "                  \"MRSFIELDSORIGINALCOOKIESINC_01_29_1998-EX-10-FRANCHISE AGREEMENT.PDF\",\n",
    "                  \"PlayboyEnterprisesInc_20090220_10-QA_EX-10.2_4091580_EX-10.2_Content License Agreement_ Marketing Agreement_ Sales-Purchase Agreement1.pdf\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a33b402-b7f5-4589-9b69-23f5162f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = \"../lab-data/\"+\"master_clauses.csv\"  \n",
    "\n",
    "df = pd.read_csv(TRN_FILE)\n",
    "\n",
    "question_list = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "113f4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_data():\n",
    "    for k in range(len(CONTRACT_FILES)):\n",
    "        df_qa = pd.melt(df[df[\"Filename\"]==CONTRACT_FILES[k]], \n",
    "                        id_vars=['Filename'], \n",
    "                        var_name='title', \n",
    "                        value_name='answer'\n",
    "                       )\n",
    "\n",
    "        df_qa = df_qa.drop(['Filename'],axis=1)\n",
    "\n",
    "        for i in range(len(df_qa)):\n",
    "            if (i%2==0):\n",
    "                question_list.append(\"What is the \"+df_qa['title'][i]+\" in the contract \"+CONTRACT_FILES[k].split('.PDF')[0]+\" ?\")\n",
    "            else:\n",
    "                answer_list.append(df_qa['answer'][i])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afb6a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_qa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0adc90a6-07dc-4fd2-9611-4baf080f076d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the Document Name in the contract BONT...</td>\n",
       "      <td>AGENCY AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the Parties in the contract BONTONSTOR...</td>\n",
       "      <td>The Bon-Ton Stores, Inc. and its associated ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the Agreement Date in the contract BON...</td>\n",
       "      <td>4/18/18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the Governing Law in the contract BONT...</td>\n",
       "      <td>Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the Most Favored Nation in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>159</td>\n",
       "      <td>What is the Liquidated Damages in the contract...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>160</td>\n",
       "      <td>What is the Warranty Duration in the contract ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>161</td>\n",
       "      <td>What is the Insurance in the contract PlayboyE...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>162</td>\n",
       "      <td>What is the Covenant Not To Sue in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>163</td>\n",
       "      <td>What is the Third Party Beneficiary in the con...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           question  \\\n",
       "0        0  What is the Document Name in the contract BONT...   \n",
       "1        1  What is the Parties in the contract BONTONSTOR...   \n",
       "2        2  What is the Agreement Date in the contract BON...   \n",
       "3        7  What is the Governing Law in the contract BONT...   \n",
       "4        8  What is the Most Favored Nation in the contrac...   \n",
       "..     ...                                                ...   \n",
       "153    159  What is the Liquidated Damages in the contract...   \n",
       "154    160  What is the Warranty Duration in the contract ...   \n",
       "155    161  What is the Insurance in the contract PlayboyE...   \n",
       "156    162  What is the Covenant Not To Sue in the contrac...   \n",
       "157    163  What is the Third Party Beneficiary in the con...   \n",
       "\n",
       "                                                answer  \n",
       "0                                     AGENCY AGREEMENT  \n",
       "1    The Bon-Ton Stores, Inc. and its associated ch...  \n",
       "2                                              4/18/18  \n",
       "3                                             Delaware  \n",
       "4                                                   No  \n",
       "..                                                 ...  \n",
       "153                                                 No  \n",
       "154                                                 No  \n",
       "155                                                Yes  \n",
       "156                                                 No  \n",
       "157                                                 No  \n",
       "\n",
       "[158 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe\n",
    "df_seed_data = pd.DataFrame()\n",
    "df_seed_data['question'] = question_list\n",
    "df_seed_data['answer'] = answer_list\n",
    "\n",
    "# remove nan answer\n",
    "df_seed_data = df_seed_data[df_seed_data.isna().answer==False]\n",
    "df_seed_data = df_seed_data.reset_index()\n",
    "df_seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f1239ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'answer'],\n",
       "    num_rows: 158\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data = df_seed_data\n",
    "\n",
    "sample = Dataset.from_pandas(df_train_data)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1568d1",
   "metadata": {},
   "source": [
    "### Prepare data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c508f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53c9a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1522715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7772dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df473c1",
   "metadata": {},
   "source": [
    "Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fa94308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6f62c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{INPUT_KEY}{sample['question']}\\n\" if sample['question'] else None\n",
    "    response = f\"{RESPONSE_KEY}{sample['answer']}\\n\"\n",
    "    #response = f\"{RESPONSE_KEY}{[[sample['routing_label']]]}\\n\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "213b6e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "\n",
      "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
      "Please provide answer to the question listed below about the important contract clauses. \n",
      "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
      "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
      "\n",
      "[Question]: What is the Unlimited/All-You-Can-Eat-License in the contract ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT ?\n",
      "\n",
      "[Response]: No\n",
      "\n",
      "[End]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "sample_p = create_prompt_formats(sample[randrange(100)])\n",
    "print(sample_p['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be5bed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b48a0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cac24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['question', 'answer', 'text', 'index'],\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f8c1f",
   "metadata": {},
   "source": [
    "Setup model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70e368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6ba46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = False\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0e82962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for qLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3291f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of GPU device and set maximum memory\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{40960}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de561378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31225a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe4578315ce433daa20c5820c3b1963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c36229c84154242898ef439a90de0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc7902cb1004baa95a6bf61bd67e90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec6630ce85e48de95854412328b5b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b888007639474628aa88af162aa1166c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66e528bbe9e4962bac70811f103a247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220267137fd24a8eb1526edce837b7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a1fb05a4ff4fee9f39f75ff161a90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f2a6b959044f2f901538094b482d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8920e146c8cb4ba68752d71e8045b5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209416d941af40399fc9d6d6d97186a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    #load_in_8bit=True,\n",
    "    device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "    #max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    token = TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load model tokenizer with the user authentication token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          token = TOKEN,\n",
    "                                         )\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set padding token as EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "655a51bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 21:32:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P0              51W / 300W |   5894MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      9221      C   ...conda3/envs/pytorch_p310/bin/python     5892MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217308b",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "539719cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 8192\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0beacc95fa4c47998aee53c25764da48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63791c55fb9f445f91d230ea2081bb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2681876abb4f8eb238d78857101e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "seed = 0\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "effae89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 158\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "967db6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 64,    #16,\n",
    "    lora_alpha = 256,    #64,\n",
    "    #target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",],\n",
    "    target_modules = ['gate_proj', 'up_proj', 'q_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73e73b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b693de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "#model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the model for training\n",
    "#model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8012daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Parameters: 4,708,372,480 || Trainable Parameters: 167,772,160 || Trainable Parameters %: 3.5632728870252848\n"
     ]
    }
   ],
   "source": [
    "# Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print information about the percentage of trainable parameters\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf96033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1  #4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 1e-5    # 1e-4 caused ocsillation...\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "#max_steps = 1000\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True  \n",
    "\n",
    "# Log every X updates steps\n",
    "epoch = 2  #5\n",
    "logging_steps = epoch*10\n",
    "\n",
    "save_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "895cf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = preprocessed_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        #gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        warmup_steps = warmup_steps,\n",
    "        #max_steps = max_steps,\n",
    "        num_train_epochs = epoch,\n",
    "        learning_rate = learning_rate,\n",
    "        fp16 = fp16,\n",
    "        logging_steps = logging_steps,\n",
    "        output_dir = output_dir,\n",
    "        #optim = optim,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps = save_steps,\n",
    "    ),\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c76572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 02:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b29739-2dc2b60b21928bfe40248260;ce99a80f-66e8-42d8-ae3f-380dbf3f5bae)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =  2147585GF\n",
      "  train_loss               =     0.2456\n",
      "  train_runtime            = 0:03:16.86\n",
      "  train_samples_per_second =      1.605\n",
      "  train_steps_per_second   =      1.605\n",
      "{'train_runtime': 196.8651, 'train_samples_per_second': 1.605, 'train_steps_per_second': 1.605, 'total_flos': 2305952305889280.0, 'train_loss': 0.2456012826554383, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "do_train = True\n",
    "\n",
    "# Launch training and log metrics\n",
    "print(\"Training...\")\n",
    "\n",
    "if do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8da8a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last checkpoint of the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b29743-02066f7f61caf2a9580c7cc7;dc048244-831f-43e3-99fc-b7b1b3335342)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "print(\"Saving last checkpoint of the model...\")\n",
    "trainer.model.save_pretrained(output_dir,\n",
    "                              token = TOKEN,\n",
    "                              trust_remote_code=True,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547eac32",
   "metadata": {},
   "source": [
    "### Test inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8311fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "tokenizer_ft = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19a7bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_train_data['question'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10f89c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "\n",
      "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
      "Please provide answer to the question listed below about the important contract clauses. \n",
      "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
      "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
      "\n",
      "[Question]: What is the Unlimited/All-You-Can-Eat-License in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT ?\n",
      "\n",
      "[Response]: \n"
     ]
    }
   ],
   "source": [
    "blurb = f\"{INTRO_BLURB}\"\n",
    "instruction = f\"{INSTRUCTION_KEY}\"\n",
    "input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9f44fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    print(\"generated_text = \", response)\n",
    "    full_text = response.split('[Response]:')[1].split('[End]')[0].strip()\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d8e77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_text =  <|begin_of_text|>What is the Unlimited/All-You-Can-Eat-License in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT? What is the Unlimited/All-You-Can-Eat-License in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT?\n",
      "The Unlimited/All-You-Can-Eat-License is not mentioned in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT. The contract is an Agency Agreement between Bonton Stores, Inc. and its agent, and it does not contain any reference to an Unlimited/All-You-Can-Eat-License. It's possible that you may be looking at the wrong contract or that the term is not relevant to the contract. If you have any further questions or concerns, please let me know and I'll be happy to help.<|eot_id|>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer, elapse_time \u001b[38;5;241m=\u001b[39m \u001b[43mLlama_Infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion = \u001b[39m\u001b[38;5;124m\"\u001b[39m, query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer = \u001b[39m\u001b[38;5;124m\"\u001b[39m, answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mElapse time = \u001b[39m\u001b[38;5;124m\"\u001b[39m, elapse_time)\n",
      "Cell \u001b[0;32mIn[76], line 27\u001b[0m, in \u001b[0;36mLlama_Infer\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     24\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m et \u001b[38;5;241m-\u001b[39m st\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text = \u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m---> 27\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[Response]:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[End]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m answer \u001b[38;5;241m=\u001b[39m full_text\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer, elapsed_time\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"Answer = \", answer, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493597b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model --- restart kernel ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0395eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "#from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d5a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2353df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744c971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b921c581",
   "metadata": {},
   "source": [
    "Load FM and Peft-load adapter then merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6047456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = False\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for qLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa79baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of GPU device and set maximum memory\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{40960}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc61a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361f467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69133618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c28fb66e0634ed5935521794ac52fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d13ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = PeftModel.from_pretrained(\n",
    "    model_ft, \n",
    "    output_dir, \n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d016c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1682ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
    "tokenizer_ft.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c17c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 22:00:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   44C    P0              52W / 300W |   8344MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     31789      C   ...conda3/envs/pytorch_p310/bin/python     8342MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404fe75d",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b70938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "986d3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACT_FILES = [\"BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\",\n",
    "                  \"ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT.PDF\",\n",
    "                  \"MRSFIELDSORIGINALCOOKIESINC_01_29_1998-EX-10-FRANCHISE AGREEMENT.PDF\",\n",
    "                  \"PlayboyEnterprisesInc_20090220_10-QA_EX-10.2_4091580_EX-10.2_Content License Agreement_ Marketing Agreement_ Sales-Purchase Agreement1.pdf\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056cb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = \"../lab-data/\"+\"master_clauses.csv\"  \n",
    "\n",
    "df = pd.read_csv(TRN_FILE)\n",
    "\n",
    "question_list = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e379c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_data():\n",
    "    for k in range(len(CONTRACT_FILES)):\n",
    "        df_qa = pd.melt(df[df[\"Filename\"]==CONTRACT_FILES[k]], \n",
    "                        id_vars=['Filename'], \n",
    "                        var_name='title', \n",
    "                        value_name='answer'\n",
    "                       )\n",
    "\n",
    "        df_qa = df_qa.drop(['Filename'],axis=1)\n",
    "\n",
    "        for i in range(len(df_qa)):\n",
    "            if (i%2==0):\n",
    "                question_list.append(\"What is the \"+df_qa['title'][i]+\" in the contract \"+CONTRACT_FILES[k].split('.PDF')[0]+\" ?\")\n",
    "            else:\n",
    "                answer_list.append(df_qa['answer'][i])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f25821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_qa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a62a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the Document Name in the contract BONT...</td>\n",
       "      <td>AGENCY AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the Parties in the contract BONTONSTOR...</td>\n",
       "      <td>The Bon-Ton Stores, Inc. and its associated ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the Agreement Date in the contract BON...</td>\n",
       "      <td>4/18/18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the Governing Law in the contract BONT...</td>\n",
       "      <td>Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the Most Favored Nation in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>159</td>\n",
       "      <td>What is the Liquidated Damages in the contract...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>160</td>\n",
       "      <td>What is the Warranty Duration in the contract ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>161</td>\n",
       "      <td>What is the Insurance in the contract PlayboyE...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>162</td>\n",
       "      <td>What is the Covenant Not To Sue in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>163</td>\n",
       "      <td>What is the Third Party Beneficiary in the con...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           question  \\\n",
       "0        0  What is the Document Name in the contract BONT...   \n",
       "1        1  What is the Parties in the contract BONTONSTOR...   \n",
       "2        2  What is the Agreement Date in the contract BON...   \n",
       "3        7  What is the Governing Law in the contract BONT...   \n",
       "4        8  What is the Most Favored Nation in the contrac...   \n",
       "..     ...                                                ...   \n",
       "153    159  What is the Liquidated Damages in the contract...   \n",
       "154    160  What is the Warranty Duration in the contract ...   \n",
       "155    161  What is the Insurance in the contract PlayboyE...   \n",
       "156    162  What is the Covenant Not To Sue in the contrac...   \n",
       "157    163  What is the Third Party Beneficiary in the con...   \n",
       "\n",
       "                                                answer  \n",
       "0                                     AGENCY AGREEMENT  \n",
       "1    The Bon-Ton Stores, Inc. and its associated ch...  \n",
       "2                                              4/18/18  \n",
       "3                                             Delaware  \n",
       "4                                                   No  \n",
       "..                                                 ...  \n",
       "153                                                 No  \n",
       "154                                                 No  \n",
       "155                                                Yes  \n",
       "156                                                 No  \n",
       "157                                                 No  \n",
       "\n",
       "[158 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe\n",
    "df_seed_data = pd.DataFrame()\n",
    "df_seed_data['question'] = question_list\n",
    "df_seed_data['answer'] = answer_list\n",
    "\n",
    "# remove nan answer\n",
    "df_seed_data = df_seed_data[df_seed_data.isna().answer==False]\n",
    "df_seed_data = df_seed_data.reset_index()\n",
    "df_seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0658da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'answer'],\n",
       "    num_rows: 158\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data = df_seed_data\n",
    "\n",
    "sample = Dataset.from_pandas(df_train_data)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431f41",
   "metadata": {},
   "source": [
    "Prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c4831e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_train_data['question'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "093176c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "\n",
      "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
      "Please provide answer to the question listed below about the important contract clauses. \n",
      "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
      "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
      "\n",
      "[Question]: What is the Joint Ip Ownership in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT ?\n",
      "\n",
      "[Response]: \n"
     ]
    }
   ],
   "source": [
    "blurb = f\"{INTRO_BLURB}\"\n",
    "instruction = f\"{INSTRUCTION_KEY}\"\n",
    "input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f85911",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09767c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    #print(\"generated_text = \", response)\n",
    "    full_text = response.split('[Response]:')[1].split('[End]')[0].strip()\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45a539a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  What is the Joint Ip Ownership in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT ? Answer =  The Joint IP Ownership clause in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT states that the parties agree to jointly own and share the intellectual property rights (IP) created during the term of the agreement. This means that both parties will have equal rights to use, modify, and distribute the IP, and any profits generated from the IP will be shared equally between the parties. The clause also specifies that the parties will work together to maintain and defend the IP, and that any disputes arising from the IP will be resolved through arbitration. \n",
      "\n",
      "Please note that the above response is based on the provided contract and may not be applicable to other contracts or situations. It is recommended to consult with a legal expert or review the contract in its entirety to ensure accurate understanding and compliance. \n",
      "\n",
      "I hope this response meets your requirements. If you need any further assistance, please feel free to ask. \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "Legal AI Assistant. <|eot_id|> \n",
      "Elapse time =  22.51023554801941\n"
     ]
    }
   ],
   "source": [
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"Answer = \", answer, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aebbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ecc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91049fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
