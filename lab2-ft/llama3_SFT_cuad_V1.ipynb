{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e0e3e2",
   "metadata": {},
   "source": [
    "## PEFT/LoRA fine tuning with  dataset\n",
    "\n",
    "Instruction fine-tuning: meta.llama3-8b-instruct-v1:0 (meta-llama/Meta-Llama-3-8B-Instruct) with LoRA, no quantization\n",
    "     \n",
    "Training: transformer trainer   \n",
    "Data: CUAD - BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\n",
    "\n",
    "2024/07/31: first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c044bf-5cc7-4154-84b1-1911ee2ae195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "#!pip install py7zr\n",
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23244385-724f-4472-8d1f-8de6f938fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade sagemaker \n",
    "#!pip install ipywidgets==7.0.0 \n",
    "#!pip install langchain==0.0.148 \n",
    "#!pip install faiss-cpu \n",
    "\n",
    "#!pip install pypdf\n",
    "#!pip install sentence_transformers\n",
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6240fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31eebe",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "#from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2028cb-361c-4c91-a3fa-34285224d83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.__version__       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64de6-aa77-469a-bd9d-66c47f3fac8b",
   "metadata": {},
   "source": [
    "### Prepare data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc2e29-ffd9-47df-971f-06b56f5aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34280b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACT_FILES = [\"BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\",\n",
    "                  \"ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT.PDF\",\n",
    "                  \"MRSFIELDSORIGINALCOOKIESINC_01_29_1998-EX-10-FRANCHISE AGREEMENT.PDF\",\n",
    "                  \"PlayboyEnterprisesInc_20090220_10-QA_EX-10.2_4091580_EX-10.2_Content License Agreement_ Marketing Agreement_ Sales-Purchase Agreement1.pdf\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33b402-b7f5-4589-9b69-23f5162f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = \"../lab-data/\"+\"master_clauses.csv\"  \n",
    "\n",
    "df = pd.read_csv(TRN_FILE)\n",
    "\n",
    "question_list = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_data():\n",
    "    for k in range(len(CONTRACT_FILES)):\n",
    "        df_qa = pd.melt(df[df[\"Filename\"]==CONTRACT_FILES[k]], \n",
    "                        id_vars=['Filename'], \n",
    "                        var_name='title', \n",
    "                        value_name='answer'\n",
    "                       )\n",
    "\n",
    "        df_qa = df_qa.drop(['Filename'],axis=1)\n",
    "\n",
    "        for i in range(len(df_qa)):\n",
    "            if (i%2==0):\n",
    "                question_list.append(\"What is the \"+df_qa['title'][i]+\" in the contract \"+CONTRACT_FILES[k].split('.PDF')[0]+\" ?\")\n",
    "            else:\n",
    "                answer_list.append(df_qa['answer'][i])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_qa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc90a6-07dc-4fd2-9611-4baf080f076d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build dataframe\n",
    "df_seed_data = pd.DataFrame()\n",
    "df_seed_data['question'] = question_list\n",
    "df_seed_data['answer'] = answer_list\n",
    "\n",
    "# remove nan answer\n",
    "df_seed_data = df_seed_data[df_seed_data.isna().answer==False]\n",
    "df_seed_data = df_seed_data.reset_index()\n",
    "df_seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1239ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = df_seed_data\n",
    "\n",
    "sample = Dataset.from_pandas(df_train_data)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1568d1",
   "metadata": {},
   "source": [
    "### Prepare data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c508f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1522715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7772dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df473c1",
   "metadata": {},
   "source": [
    "Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa94308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{INPUT_KEY}{sample['question']}\\n\" if sample['question'] else None\n",
    "    response = f\"{RESPONSE_KEY}{sample['answer']}\\n\"\n",
    "    #response = f\"{RESPONSE_KEY}{[[sample['routing_label']]]}\\n\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "sample_p = create_prompt_formats(sample[randrange(100)])\n",
    "print(sample_p['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['question', 'answer', 'text', 'index'],\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f8c1f",
   "metadata": {},
   "source": [
    "Setup model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = False\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e82962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for qLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of GPU device and set maximum memory\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{40960}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de561378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = <your token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31225a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    #load_in_8bit=True,\n",
    "    device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "    #max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    token = TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load model tokenizer with the user authentication token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          token = TOKEN,\n",
    "                                         )\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set padding token as EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217308b",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539719cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "seed = 0\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967db6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 64,    #16,\n",
    "    lora_alpha = 256,    #64,\n",
    "    #target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",],\n",
    "    target_modules = ['gate_proj', 'up_proj', 'q_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e73b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b693de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "#model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the model for training\n",
    "#model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8012daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print information about the percentage of trainable parameters\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1  #4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 1e-5    # 1e-4 caused ocsillation...\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "#max_steps = 1000\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True  \n",
    "\n",
    "# Log every X updates steps\n",
    "epoch = 2  #5\n",
    "logging_steps = epoch*10\n",
    "\n",
    "save_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = preprocessed_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        #gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        warmup_steps = warmup_steps,\n",
    "        #max_steps = max_steps,\n",
    "        num_train_epochs = epoch,\n",
    "        learning_rate = learning_rate,\n",
    "        fp16 = fp16,\n",
    "        logging_steps = logging_steps,\n",
    "        output_dir = output_dir,\n",
    "        #optim = optim,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps = save_steps,\n",
    "    ),\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "do_train = True\n",
    "\n",
    "# Launch training and log metrics\n",
    "print(\"Training...\")\n",
    "\n",
    "if do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"Saving last checkpoint of the model...\")\n",
    "trainer.model.save_pretrained(output_dir,\n",
    "                              token = TOKEN,\n",
    "                              trust_remote_code=True,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547eac32",
   "metadata": {},
   "source": [
    "### Test inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8311fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "tokenizer_ft = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_train_data['question'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f44fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    #print(\"generated_text = \", response)\n",
    "    full_text = response.split('[Response]:')[1].split('[End]')[0].strip()\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"Answer = \", answer, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493597b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model --- restart kernel ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0395eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "#from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d5a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./ft_model_llama3-8b_instruct_cuad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2353df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize static strings for the prompt template\n",
    "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n'\n",
    "\n",
    "INSTRUCTION_KEY = \"\"\"\n",
    "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
    "Please provide answer to the question listed below about the important contract clauses. \n",
    "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
    "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
    "\"\"\"\n",
    "\n",
    "INPUT_KEY = '[Question]: '\n",
    "RESPONSE_KEY = '[Response]: '\n",
    "END_KEY = \"[End]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744c971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b921c581",
   "metadata": {},
   "source": [
    "Load FM and Peft-load adapter then merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6047456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = False\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for qLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa79baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of GPU device and set maximum memory\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{40960}MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc61a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361f467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69133618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b090de296b4ae684362a954d7ff210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d13ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = PeftModel.from_pretrained(\n",
    "    model_ft, \n",
    "    output_dir, \n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d016c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1682ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
    "tokenizer_ft.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c17c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 05:55:31 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   40C    P0              51W / 300W |   8344MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3595      C   ...conda3/envs/pytorch_p310/bin/python     8342MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404fe75d",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b70938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "986d3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACT_FILES = [\"BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\",\n",
    "                  \"ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT.PDF\",\n",
    "                  \"MRSFIELDSORIGINALCOOKIESINC_01_29_1998-EX-10-FRANCHISE AGREEMENT.PDF\",\n",
    "                  \"PlayboyEnterprisesInc_20090220_10-QA_EX-10.2_4091580_EX-10.2_Content License Agreement_ Marketing Agreement_ Sales-Purchase Agreement1.pdf\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056cb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_FILE = \"../lab-data/\"+\"master_clauses.csv\"  \n",
    "\n",
    "df = pd.read_csv(TRN_FILE)\n",
    "\n",
    "question_list = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e379c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_data():\n",
    "    for k in range(len(CONTRACT_FILES)):\n",
    "        df_qa = pd.melt(df[df[\"Filename\"]==CONTRACT_FILES[k]], \n",
    "                        id_vars=['Filename'], \n",
    "                        var_name='title', \n",
    "                        value_name='answer'\n",
    "                       )\n",
    "\n",
    "        df_qa = df_qa.drop(['Filename'],axis=1)\n",
    "\n",
    "        for i in range(len(df_qa)):\n",
    "            if (i%2==0):\n",
    "                question_list.append(\"What is the \"+df_qa['title'][i]+\" in the contract \"+CONTRACT_FILES[k].split('.PDF')[0]+\" ?\")\n",
    "            else:\n",
    "                answer_list.append(df_qa['answer'][i])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f25821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_qa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a62a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the Document Name in the contract BONT...</td>\n",
       "      <td>AGENCY AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the Parties in the contract BONTONSTOR...</td>\n",
       "      <td>The Bon-Ton Stores, Inc. and its associated ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the Agreement Date in the contract BON...</td>\n",
       "      <td>4/18/18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the Governing Law in the contract BONT...</td>\n",
       "      <td>Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the Most Favored Nation in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>159</td>\n",
       "      <td>What is the Liquidated Damages in the contract...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>160</td>\n",
       "      <td>What is the Warranty Duration in the contract ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>161</td>\n",
       "      <td>What is the Insurance in the contract PlayboyE...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>162</td>\n",
       "      <td>What is the Covenant Not To Sue in the contrac...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>163</td>\n",
       "      <td>What is the Third Party Beneficiary in the con...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           question  \\\n",
       "0        0  What is the Document Name in the contract BONT...   \n",
       "1        1  What is the Parties in the contract BONTONSTOR...   \n",
       "2        2  What is the Agreement Date in the contract BON...   \n",
       "3        7  What is the Governing Law in the contract BONT...   \n",
       "4        8  What is the Most Favored Nation in the contrac...   \n",
       "..     ...                                                ...   \n",
       "153    159  What is the Liquidated Damages in the contract...   \n",
       "154    160  What is the Warranty Duration in the contract ...   \n",
       "155    161  What is the Insurance in the contract PlayboyE...   \n",
       "156    162  What is the Covenant Not To Sue in the contrac...   \n",
       "157    163  What is the Third Party Beneficiary in the con...   \n",
       "\n",
       "                                                answer  \n",
       "0                                     AGENCY AGREEMENT  \n",
       "1    The Bon-Ton Stores, Inc. and its associated ch...  \n",
       "2                                              4/18/18  \n",
       "3                                             Delaware  \n",
       "4                                                   No  \n",
       "..                                                 ...  \n",
       "153                                                 No  \n",
       "154                                                 No  \n",
       "155                                                Yes  \n",
       "156                                                 No  \n",
       "157                                                 No  \n",
       "\n",
       "[158 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe\n",
    "df_seed_data = pd.DataFrame()\n",
    "df_seed_data['question'] = question_list\n",
    "df_seed_data['answer'] = answer_list\n",
    "\n",
    "# remove nan answer\n",
    "df_seed_data = df_seed_data[df_seed_data.isna().answer==False]\n",
    "df_seed_data = df_seed_data.reset_index()\n",
    "df_seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0658da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'answer'],\n",
       "    num_rows: 158\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data = df_seed_data\n",
    "\n",
    "sample = Dataset.from_pandas(df_train_data)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431f41",
   "metadata": {},
   "source": [
    "Prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c4831e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_train_data['question'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "093176c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "\n",
      "[Instruction]: You are a legal AI assistant reviwing commercial contracts. \n",
      "Please provide answer to the question listed below about the important contract clauses. \n",
      "The questions are provided after the [Question] tag, present your answer after the [Response] tag. \n",
      "DO NOT put any premables in the response. If you don't know the answer, just say I don't know, DO NOT make up the answers' \n",
      "\n",
      "[Question]: What is the Joint Ip Ownership in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT ?\n",
      "\n",
      "[Response]: \n"
     ]
    }
   ],
   "source": [
    "blurb = f\"{INTRO_BLURB}\"\n",
    "instruction = f\"{INSTRUCTION_KEY}\"\n",
    "input_context = f'{INPUT_KEY}{query}\\n\\n{RESPONSE_KEY}'\n",
    "\n",
    "prompt = blurb+'\\n'+instruction+'\\n'+input_context\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f85911",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09767c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def Llama_Infer(prompt):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    batch = tokenizer_ft(prompt, return_tensors=\"pt\")\n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        output = model_ft.generate(input_ids, \n",
    "                                    max_new_tokens=256,\n",
    "                                    do_sample=True,\n",
    "                                    temperature = 0.01,\n",
    "                                    pad_token_id=tokenizer_ft.eos_token_id,\n",
    "                                    )[0]       \n",
    "\n",
    "        response = tokenizer_ft.decode(output)\n",
    "\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    \n",
    "    #print(\"generated_text = \", response)\n",
    "    full_text = response.split('[Response]:')[1].split('[End]')[0].strip()\n",
    "    answer = full_text\n",
    "    \n",
    "    return answer, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45a539a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  What is the Joint Ip Ownership in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT ? Answer =  The Joint IP Ownership clause in the contract BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT states that the parties agree to jointly own and share the intellectual property rights (IP) created during the term of the agreement. This means that both parties will have equal rights to use, modify, and distribute the IP, and any profits generated from the IP will be shared equally. The clause also specifies that the parties will work together to maintain and defend the IP, and that any disputes arising from the IP will be resolved through arbitration. \n",
      "\n",
      "Please note that the above response is based on the provided contract and may not be applicable to other contracts or situations. It is recommended to consult with a legal expert or review the contract in its entirety before making any decisions or taking any actions. \n",
      "\n",
      "I hope this response meets your requirements. If you need any further assistance, please feel free to ask. \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "Legal AI Assistant. <|eot_id|> \n",
      "Elapse time =  20.36516261100769\n"
     ]
    }
   ],
   "source": [
    "answer, elapse_time = Llama_Infer(prompt)\n",
    "print(\"Question = \", query, \"Answer = \", answer, \"\\nElapse time = \", elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aebbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ecc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91049fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
