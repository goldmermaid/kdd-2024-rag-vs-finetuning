{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: This notebooks demonstrates how to compare a fine-tuned model and the original pre-trained model using RAG by plotting their evaluation results in a radar plot and adding cost analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.1.1 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU pinecone-client==2.2.1 ipywidgets==7.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mkl-fft 1.3.8 requires mkl, which is not installed.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.1.4 which is incompatible.\n",
      "awscli 1.32.3 requires botocore==1.34.3, but you have botocore 1.34.155 which is incompatible.\n",
      "awscli 1.32.3 requires s3transfer<0.10.0,>=0.9.0, but you have s3transfer 0.10.2 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fmeval --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages needed for plotting\n",
    "! pip install -U kaleido --quiet\n",
    "! pip install plotly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here get the metrics from lab1 and lab2 notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualize results pre-trained vs fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the results and visualize them as radar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for loading the results (obs: for llama 2 7B chat doesn't have the key accuracy in the json file generated\n",
    "def load_results(models):\n",
    "    accuracy_results = []\n",
    "    for model in models:\n",
    "        #print(\">>>model\",model,\"<<<\")\n",
    "        file = f'example_results/{model}.json'\n",
    "        with open(file, 'r') as f:\n",
    "            res = json.load(f)\n",
    "            #print(\">>>res\",res,\"<<<\")\n",
    "            #print(\">>>res[0]\",res[2],\"<<<\")\n",
    "            #for accuracy_eval in res['accuracy']:\n",
    "            for accuracy_eval in res:\n",
    "                for accuracy_scores in accuracy_eval[\"dataset_scores\"]:\n",
    "                    accuracy_results.append(\n",
    "                        {'model': model, 'evaluation': 'accuracy', 'dataset': accuracy_eval[\"dataset_name\"],\n",
    "                         'metric': accuracy_scores[\"name\"], 'value': accuracy_scores[\"value\"]})\n",
    "        \n",
    "    accuracy_results_df = pd.DataFrame(accuracy_results)\n",
    "    return accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting the results\n",
    "def visualize_radar(results_df, dataset):\n",
    "    # aggregate 3 datasets into 1 by taking mean across datasets\n",
    "    if dataset == 'all':\n",
    "       mean_across_datasets = results_df.drop('evaluation', axis=1).groupby(['model', 'metric']).describe()['value']['mean']\n",
    "       results_df = pd.DataFrame(mean_across_datasets).reset_index().rename({'mean':'value'}, axis=1)\n",
    "    # plot a single dataset\n",
    "    else:\n",
    "        results_df = results_df[results_df['dataset'] == dataset]\n",
    "    \n",
    "    fig = px.line_polar(results_df, r='value', theta='metric', color='model', line_close=True) \n",
    "    xlim = 1\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, xlim],\n",
    "            )),\n",
    "        margin=dict(l=150, r=0, t=100, b=80)\n",
    "    )\n",
    "\n",
    "    \n",
    "    title =  'Average Performance over 3 QA Datasets' if dataset == 'all' else dataset\n",
    "    fig.update_layout(\n",
    "            title=dict(text=title, font=dict(size=20), yref='container')\n",
    "        )\n",
    "    \n",
    "    directory = \"example_results\"\n",
    "    fig.show()\n",
    "    fig.write_image(f\"{directory}/radarplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_id_base + \"_base\", model_id_instruct + \"_instruct\"]\n",
    "results_df = load_results(models)\n",
    "visualize_radar(results_df, dataset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Retrieval-Augmented Generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Ref. notebook (https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-chat-completion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use [**Llama-2-7b**](https://ai.meta.com/llama/) to answer questions using a library of documents as a reference, by using document embeddings and retrieval. Unlike other RAG solutions, embeddings will be generated and combined with the embedding model to identify the nearest neighbors, all from a single endpoint in this solution.\n",
    "\n",
    "\n",
    "To perform inference on the [Llama models](https://ai.meta.com/llama/), you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from this [webpage](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from rag notebook use below the same code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text using PyMuPDF\n",
    "def extract_text_from_pdf_mupdf(pdf_path):\n",
    "    text = \"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the PDF using PyMuPDF\n",
    "pdf_text_mupdf = extract_text_from_pdf_mupdf(\"../lab-data/BONTONSTORESINC_04_20_2018-EX-99.3-AGENCY AGREEMENT.PDF\")\n",
    "pdf_text_mupdf[:2000]  # Displaying the first 2000 characters to get an overview of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunk text into smaller segments with a specified chunk size and overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be chunked.\n",
    "    - chunk_size (int): The size of each chunk.\n",
    "    - overlap (int): The number of characters that overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError(\"Chunk size must be greater than overlap\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "\n",
    "    while start < len(text):\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "        end = start + chunk_size\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(pdf_text_mupdf, 1024, 256)\n",
    "print(len(chunks))\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "client = boto3.client('bedrock-runtime', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed text\n",
    "def embed_text(input_text):\n",
    "    # Create the request payload\n",
    "    payload = {\n",
    "        \"inputText\": input_text,\n",
    "        \"dimensions\": 512,  # Specify the desired dimension size\n",
    "        \"normalize\": True  # Whether to normalize the output embeddings\n",
    "    }\n",
    "\n",
    "    # Invoke the model\n",
    "    response = client.invoke_model(\n",
    "        body=json.dumps(payload),\n",
    "        modelId='amazon.titan-embed-text-v2:0',  # Specify the Titan embedding model\n",
    "        accept='application/json',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Get the embedding result\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    return embedding\n",
    "\n",
    "# Print the embedding\n",
    "print(embed_text(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyepsilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh ../setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyepsilla import vectordb\n",
    "## connect to vectordb\n",
    "db = vectordb.Client(\n",
    "  host='localhost',\n",
    "  port='8888'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.unload_db(\"kdd_lab1_rag\")\n",
    "db.load_db(db_name=\"kdd_lab1_rag\", db_path=\"/tmp/kdd_lab1_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.use_db(db_name=\"kdd_lab1_rag\")\n",
    "db.create_table(\n",
    "  table_name=\"NaiveRAG\",\n",
    "  table_fields=[\n",
    "    {\"name\": \"ID\", \"dataType\": \"INT\", \"primaryKey\": True},\n",
    "    {\"name\": \"Doc\", \"dataType\": \"STRING\"},\n",
    "    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 512}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\n",
    "        \"ID\": index,\n",
    "        \"Doc\": text,\n",
    "        \"Embedding\": embed_text(text)\n",
    "    }\n",
    "    for index, text in enumerate(chunks)\n",
    "]\n",
    "records[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert(\"NaiveRAG\", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    # Create the request payload\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0,  # Adjust the randomness of the output\n",
    "        \"max_gen_len\": 128\n",
    "    }\n",
    "\n",
    "    # Initialize the Bedrock runtime client\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "    # Invoke the model\n",
    "    response = client.invoke_model(\n",
    "        modelId='meta.llama3-8b-instruct-v1:0',\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    byte_response = response['body'].read()\n",
    "    json_string = byte_response.decode('utf-8')\n",
    "\n",
    "    # Get the chat response\n",
    "    response_body = json.loads(json_string)\n",
    "    chat_response = response_body.get('generation')\n",
    "\n",
    "    return chat_response\n",
    "\n",
    "# Example usage\n",
    "input_text = \"How are you?\"\n",
    "prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{input_text}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_retriever(question, top_k):\n",
    "    code, resp = db.query(\n",
    "        table_name=\"NaiveRAG\",\n",
    "        query_field=\"Embedding\",\n",
    "        query_vector=embed_text(question),\n",
    "        limit=top_k\n",
    "    )\n",
    "    return resp[\"result\"]\n",
    "basic_retriever(\"What's the agreement date?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rag(question):\n",
    "    docs = basic_retriever(question, 5)\n",
    "    docs_str = \"------------------------\\n\"\n",
    "    for doc in docs:\n",
    "        docs_str += doc[\"Doc\"] + \"------------------------\\n\"\n",
    "    prompt = f\"\"\"\n",
    "<s>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the documents below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "\n",
    "<documents>\n",
    "{docs_str}\n",
    "</documents>\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "    return generate(prompt)\n",
    "\n",
    "naive_rag(\"What's the agreement date?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rag notebook until here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run cost experiment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run prompt experiment\n",
    "after defined the performance baseline for fine-tuned model, we now run and experinebt to measure the input token size reduction when using fine-tuned model compared with pre-trained using RAG.\n",
    "The goal is the measure the savings in token input use that we have by not using context information in RAG solutions. \n",
    "#### Question to answer\n",
    "When is it worth to move to RAG instead of fine-tune the pre-trained model? considering input token costs only as this is the big cost difference between the same solutioj using RAg or fine-tuning.\n",
    "RAG has an extra cost related to the context added to the prompt. \n",
    "We need to measure this difference on average and use this information to support a cost based decison related to use RAg or fine-tune.\n",
    "basically, considering a break even approach: what is the fine-tuned frequency that would compensate for the same RAG cost, considering a defined fine-tune frequency? While keeping at least the same performance of fine-tuned model\n",
    "We need to perfom the steps:\n",
    "1. define a batch of at least 30 prompts and compute the mean size of input\n",
    "2. run the pre-trained model + RAG with these prompts\n",
    "    2.1 get the context retrieved and measure the mean size returned. This is the average difference of tokens added related to the fine-tun used (with same prompts)\n",
    "3. Simulate an example of fine-tuned model costs considering a basic AWS architecture (OpenSearch, SageMaker, S3, etc) and calculate total cost for each traning\n",
    "4. Create a break even plot based on tese 2 costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place all of this logic into a single RAG query function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASS: using embedding model bellow: \"huggingface-sentencesimilarity-gte-small\"\n",
    "import spacy \n",
    "def rag_query(question: str) -> str:\n",
    "    # Get nearest neighbor THIS WIORKS FOR EMBEDDING MODEL\n",
    "    payload_nearest_neighbour = {\n",
    "        \"queries\": question,\n",
    "        \"top_k\": 5,\n",
    "        \"mode\": \"nn_train_data\",\n",
    "        \"return_text\": True,\n",
    "    } \n",
    "    \n",
    "    response = predictor_nn.predict(payload_nearest_neighbour)[0]\n",
    "    #print(\"\\n>>>resp\", response, \" \\n>>>\")\n",
    "    # get contexts\n",
    "    contexts = [ans[\"text\"] for ans in response]\n",
    "    #print(\"\\n>>>contexts\", contexts, \" \\n>>>\")\n",
    "    # build the multiple contexts string\n",
    "    context_str = construct_context(contexts=contexts)\n",
    "    # counting number of tokens of context text\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    context_tokens = nlp(context_str) \n",
    "    #print(\"\\n>>>context_str\", context_str, \" \\n>>>\")\n",
    "    # create our retrieval augmented prompt\n",
    "    payload = create_payload(question, context_str)\n",
    "    #print(\">>>>>>>>>>IN payload['inputs'][0]\",payload['inputs'][0], \"<<<<<<<<<<<\")\n",
    "    inputs = format_messages(payload['inputs'][0])\n",
    "    #print(\">>>>>>>>>> OUT inputs IS OK!\",inputs, \"<<<<<<<<<<<\")\n",
    "    payload['inputs'] = inputs\n",
    "    #print(\">>>>>>>>>>payload built \",payload, \"<<<<<<<<<<<\")\n",
    "    # make prediction\n",
    "    out = predictor_base.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    #print(\"\\n>>>out\", out, \" \\n<<<\")\n",
    "    return len(context_tokens), out[0]['generated_text']\n",
    "    #return out[0][\"generation\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "GeeksforGeeks\n",
      "is\n",
      "a\n",
      "one\n",
      "stop\n",
      "learning\n",
      "destination\n",
      "for\n",
      "geeks\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# First we need to import spacy ADD ABOVE!!\n",
    "import spacy \n",
    "\n",
    "# Creating blank language object then \n",
    "# tokenizing words of the sentence \n",
    "nlp = spacy.blank(\"en\") \n",
    "\n",
    "doc = nlp(\"GeeksforGeeks is a one stop learning destination for geeks.\") \n",
    "print (len(doc))\n",
    "for token in doc: \n",
    "\tprint(token) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASS: TODO create loop to run 30 questions from list and claculate average context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of payload from here (https://studio-d-m1tidsvb7ha6.studio.us-east-1.sagemaker.aws/JumpStart/SageMakerPublicHub/Model/huggingface-sentencesimilarity-gte-small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now ask the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n"
     ]
    }
   ],
   "source": [
    "num_tokens, out_prompt = rag_query(\"Does SageMaker support spot instances?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/KDD2024\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeffWhat is Amazon SageMaker?'],\n",
       " ['In which Regions is Amazon SageMaker available?'],\n",
       " ['What is the service availability of Amazon SageMaker?'],\n",
       " ['How does Amazon SageMaker secure my code?'],\n",
       " ['What security measures does Amazon SageMaker have?'],\n",
       " ['Does Amazon SageMaker use or share models',\n",
       "  ' training data',\n",
       "  ' or algorithms?'],\n",
       " ['How am I charged for Amazon SageMaker?'],\n",
       " ['How can I optimize my Amazon SageMaker costs',\n",
       "  ' such as detecting and stopping idle resources in order to avoid unnecessary charges?'],\n",
       " ['What if I have my own notebook', ' training', ' or hosting environment?'],\n",
       " ['Is R supported with Amazon SageMaker?'],\n",
       " ['How can I check for imbalances in my model?'],\n",
       " ['What kind of bias does Amazon SageMaker Clarify detect?'],\n",
       " ['How does Amazon SageMaker Clarify improve model explainability?'],\n",
       " ['What is Amazon SageMaker Studio?'],\n",
       " ['What is RStudio on Amazon SageMaker?'],\n",
       " ['How does Amazon SageMaker Studio pricing work?'],\n",
       " ['In which Regions is Amazon SageMaker Studio supported?'],\n",
       " ['What ML governance tools does Amazon SageMaker provide?'],\n",
       " ['What does Amazon SageMaker Role Manager do?'],\n",
       " ['What does Amazon SageMaker Model Cards do?'],\n",
       " ['What does Amazon SageMaker Model Dashboard do?\\xa0'],\n",
       " ['What is Amazon SageMaker Autopilot?'],\n",
       " ['What built-in algorithms are supported in Amazon SageMaker Autopilot?'],\n",
       " ['Can I stop an Amazon SageMaker Autopilot job manually?'],\n",
       " ['How do I get started with Amazon SageMaker quickly?'],\n",
       " ['Which open-source models are supported with Amazon SageMaker JumpStart?'],\n",
       " ['What solutions come pre-built with Amazon SageMaker JumpStart?'],\n",
       " ['How can I share ML artifacts with others within my organization?'],\n",
       " ['Why should I use Amazon SageMaker JumpStart to share ML artifacts with others within my organization?'],\n",
       " ['How does Amazon SageMaker JumpStart pricing work?'],\n",
       " ['What is Amazon SageMaker Canvas?']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "file = open(\"example_results/sagemaker_questions.csv\", \"r\")\n",
    "list_of_questions = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "list_of_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\n",
      "136\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "97\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "148\n",
      "With maximum sequence length 1000, selected top 1 document sections: \n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "128\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "135\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "135\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "186\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "135\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Yes, R is supported with Amazon SageMaker. You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, enabling ML practitioners to build, train, tune, and deploy R models.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "185\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "186\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "88\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "135\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\n",
      "136\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "186\n",
      "With maximum sequence length 1000, selected top 1 document sections: \n",
      "Amazon SageMaker does not use or share customer models, training data, or algorithms. We know that customers care deeply about privacy and data security. That's why AWS gives you ownership and control over your content through simple, powerful tools that allow you to determine where your content will be stored, secure your content in transit and at rest, and manage your access to AWS services and resources for your users. We also implement responsible and sophisticated technical and physical controls that are designed to prevent unauthorized access to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\n",
      "143\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\n",
      "148\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n",
      "With maximum sequence length 1000, selected top 3 document sections: \n",
      "You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page and the Amazon SageMaker Pricing calculator for details.\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "There are several best practices you can adopt to optimize your Amazon SageMaker resource utilization. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post.\n",
      "186\n",
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
      "Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "l_tokens = []\n",
    "for q in list_of_questions:\n",
    "    num_tokens, out_prompt = rag_query(str(q))\n",
    "    l_tokens.append(num_tokens)\n",
    "    print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # context tokens over 31 questions: 155.39\n"
     ]
    }
   ],
   "source": [
    "#print(l_tokens)\n",
    "print(\"Average # context tokens over %2d questions: %.2f\" % (len(l_tokens), sum(l_tokens) / len(l_tokens)))   # print integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost analysis Sessions here\n",
    "#### costs connsiderations\n",
    "+ common to FT and RAG will not be considered (Amazon API Gateway (WebSocket)), Amazon CloudFront, Amazon CloudWatch, __Amazon DynamoDB (probably consider for RAG too?)__, AWS Lambda\n",
    "+ only input token costs for fine-tuning and RAG\n",
    "+ knowledge base costs for RAG\n",
    "#### costs considered: queries per day, number docs RAG, number of tokens? \n",
    "(https://docs.aws.amazon.com/solutions/latest/generative-ai-application-builder-on-aws/cost.html)\n",
    "or this simplified external costs for the model: ref. (https://aws.amazon.com/bedrock/pricing/) Amazon Titan Embeddings\tN/A\t\n",
    "##### ref. Sample costs for a text-based proof of concept\n",
    "Ex. Amazon Bedrock (Titan Text Express) ->  Assumptions for 100 interactions per day:\n",
    "+ Monthly cost for 150K input tokens per day = $3.60\n",
    "+ Monthly cost for 16K output tokens per day = $0.768\n",
    "Ex. kendra for RAG:\n",
    " + 0-8,000 queries a day and up to 100,000 documents with Amazon Kendra Enterprise Edition with 0-50 data sources = $1,008.00 \n",
    "\n",
    "#### simulate FT costs:\n",
    "#### simulate RAG costs\n",
    "#### Breakeven analysis\n",
    "#### Putting all together performancfe and cost tables and plots to support cost comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask questions about things that are out of context (not contained within our dataset). From this we expect the model to *not* hallucinate and honestly tell us that it does not know the answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### accessing the ft pre-trained end point and make prediscion with batch of 30 prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
