{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4480da3",
   "metadata": {},
   "source": [
    "![image](../images/kdd24-logo-small.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cfc0",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial\n",
    "## Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices\n",
    "### Lab 1.2: Advanced RAG\n",
    "#### Summary: \n",
    "This lab focused on enhancing the Retrieval-Augmented Generation (RAG) process by implementing and comparing four advanced optimizations against a baseline Naive RAG approach. The optimizations were:\n",
    "\n",
    "- Sentence Chunking\n",
    "- Hypothetical Question Generation\n",
    "- Query Decomposition\n",
    "- Hypothetical Question + Query Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233d8df",
   "metadata": {},
   "source": [
    "#### Load the naive RAG performace as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb263f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_barchart(series_names, series_data, x_labels):\n",
    "    \"\"\"\n",
    "    Plot bar charts for multiple series with x-axis labels and legends.\n",
    "\n",
    "    :param series_names: List of names for each data series (used as legends).\n",
    "    :param series_data: List of lists, where each inner list is a series of double numbers.\n",
    "    :param x_labels: List of labels for the x-axis.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Number of series to plot\n",
    "    num_series = len(series_data)\n",
    "    \n",
    "    # Bar width depends on the number of series\n",
    "    bar_width = 0.8 / num_series\n",
    "    \n",
    "    # X-axis positions for the bars\n",
    "    x = range(len(x_labels))\n",
    "    \n",
    "    # Plot each series\n",
    "    for i, (name, data) in enumerate(zip(series_names, series_data)):\n",
    "        plt.bar([pos + i * bar_width for pos in x], data, bar_width, label=name)\n",
    "    \n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Scores by Series')\n",
    "    plt.xticks([pos + bar_width * (num_series / 2 - 0.5) for pos in x], x_labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Display the chart\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "file_path = '../lab-data/naive_rag_result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "metrics = df.columns[-3:]\n",
    "\n",
    "# Prepare the data\n",
    "series_names = [f\"Naive RAG {name}\" for name in metrics.tolist()]\n",
    "series_data = [df[metric].tolist() for metric in metrics]\n",
    "\n",
    "# Plot individual series bar charts\n",
    "for i in range(3):\n",
    "    plot_barchart([series_names[i]], [series_data[i]], list(range(len(series_data[i]))))\n",
    "\n",
    "# Calculate the average for each metric\n",
    "average_scores = [sum(data) / len(data) for data in series_data]\n",
    "\n",
    "# Plot the average scores bar chart\n",
    "plot_barchart(['Naive RAG Average Scores'], [average_scores], metrics.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text using PyMuPDF\n",
    "def extract_text_from_pdf_mupdf(pdf_path):\n",
    "    text = \"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e45da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the PDF using PyMuPDF\n",
    "pdf_text_mupdf = extract_text_from_pdf_mupdf(\"../lab-data/ENERGOUSCORP_03_16_2017-EX-10.24-STRATEGIC ALLIANCE AGREEMENT.PDF\").replace('\\xa0', ' ')\n",
    "pdf_text_mupdf  # Displaying the first 2000 characters to get an overview of the content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c512d4",
   "metadata": {},
   "source": [
    "#### Sentence Chunking + Sentence Window Retrieval Optimization\n",
    "\n",
    "Purpose: Improve the retrieval accuracy by breaking down documents into smaller, more focused chunks (typically sentences).\n",
    "\n",
    "Implementation: Documents are split into sentences, and each sentence is embedded separately. During the retrieval phase, queries are matched against these sentence-level embeddings, and then retrieve the surrounding sentences (similar idea as sliding window) to maintain the context cohesiveness.\n",
    "\n",
    "Outcome: This technique generally improves the precision of retrieved content, ensuring that the most relevant parts of the documents are used in generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_chunker(text):\n",
    "    \"\"\"\n",
    "    Chunk text into sentences.\n",
    "    \"\"\"\n",
    "    return [f\"{sentence}.\" for sentence in text.split(\".\")]\n",
    "\n",
    "sentences = sentence_chunker(pdf_text_mupdf)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_window_builder(sentences, window_size):\n",
    "    def sentence_length(sentence):\n",
    "        \"\"\"Helper function to calculate the length of a sentence.\"\"\"\n",
    "        return len(sentence)\n",
    "\n",
    "    windows = []\n",
    "    window_left_index = []\n",
    "    window_right_index = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        current_sentence = sentences[i]\n",
    "        current_length = sentence_length(current_sentence)\n",
    "        left_context = []\n",
    "        right_context = []\n",
    "        total_length = sentence_length(current_sentence)\n",
    "        \n",
    "        # Expand left context\n",
    "        left_index = i - 1\n",
    "        while left_index >= 0 and total_length + sentence_length(sentences[left_index]) - current_length // 2 <= window_size // 2:\n",
    "            left_context.insert(0, sentences[left_index])\n",
    "            total_length += sentence_length(sentences[left_index])\n",
    "            left_index -= 1\n",
    "        \n",
    "        # Expand right context\n",
    "        right_index = i + 1\n",
    "        while right_index < len(sentences) and total_length + sentence_length(sentences[right_index]) - current_length // 2 <= window_size:\n",
    "            right_context.append(sentences[right_index])\n",
    "            total_length += sentence_length(sentences[right_index])\n",
    "            right_index += 1\n",
    "        \n",
    "        # Build the window for the current sentence\n",
    "        window = left_context + [current_sentence] + right_context\n",
    "        windows.append(' '.join(window))\n",
    "        window_left_index.append(left_index + 1)\n",
    "        window_right_index.append(right_index - 1)\n",
    "    \n",
    "    return windows, window_left_index, window_right_index\n",
    "\n",
    "window_size = 1024\n",
    "windows, window_left_index, window_right_index = sentence_window_builder(sentences, window_size)\n",
    "for i, w in enumerate(result):\n",
    "    print(f\"Window {i+1}: {w}\")\n",
    "    print(f\"Window Range: [{window_left_index[i]}: {window_right_index[i]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e497481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyepsilla import vectordb\n",
    "## connect to vectordb\n",
    "db = vectordb.Client(\n",
    "  host='localhost',\n",
    "  port='8888'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a22eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.unload_db(\"kdd_lab1_advanced_rag\")\n",
    "db.load_db(db_name=\"kdd_lab1_advanced_rag\", db_path=\"/tmp/kdd_lab1_advanced_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.use_db(db_name=\"kdd_lab1_advanced_rag\")\n",
    "db.drop_table(\"SentenceWindow\")\n",
    "db.create_table(\n",
    "  table_name=\"SentenceWindow\",\n",
    "  table_fields=[\n",
    "    {\"name\": \"ID\", \"dataType\": \"INT\", \"primaryKey\": True},\n",
    "    {\"name\": \"Sentence\", \"dataType\": \"STRING\"},\n",
    "    {\"name\": \"Left\", \"dataType\": \"INT\"},\n",
    "    {\"name\": \"Right\", \"dataType\": \"INT\"},\n",
    "    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 512}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "client = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "# Function to embed text\n",
    "def embed_text(input_text):\n",
    "    # Create the request payload\n",
    "    payload = {\n",
    "        \"inputText\": input_text,\n",
    "        \"dimensions\": 512,  # Specify the desired dimension size\n",
    "        \"normalize\": True  # Whether to normalize the output embeddings\n",
    "    }\n",
    "\n",
    "    # Invoke the model\n",
    "    response = client.invoke_model(\n",
    "        body=json.dumps(payload),\n",
    "        modelId='amazon.titan-embed-text-v2:0',  # Specify the Titan embedding model\n",
    "        accept='application/json',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Get the embedding result\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    return embedding\n",
    "\n",
    "# Print the embedding\n",
    "print(embed_text(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\n",
    "        \"ID\": index,\n",
    "        \"Sentence\": sentences[index],\n",
    "        \"Left\": window_left_index[index],\n",
    "        \"Right\": window_right_index[index],\n",
    "        \"Embedding\": embed_text(sentences[index])\n",
    "    }\n",
    "    for index in range(len(sentences))\n",
    "]\n",
    "records[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fee036",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert(\"SentenceWindow\", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81585262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_window_retriever(table_name, question, top_k):\n",
    "    code, resp = db.query(\n",
    "        table_name=table_name,\n",
    "        query_field=\"Embedding\",\n",
    "        query_vector=embed_text(question),\n",
    "        limit=top_k\n",
    "    )\n",
    "    id_list = []\n",
    "    for item in resp[\"result\"]:\n",
    "        for id in range(item[\"Left\"], item[\"Right\"] + 1):\n",
    "            if id not in id_list:  # Check if the ID is already in the list\n",
    "                id_list.append(id)  # Append the ID if it's not a duplicate\n",
    "\n",
    "    code, resp = db.get(\n",
    "        table_name=table_name,\n",
    "        response_fields=[\"Sentence\"],\n",
    "        primary_keys=id_list\n",
    "    )\n",
    "    return id_list, [item[\"Sentence\"] for item in resp[\"result\"]]\n",
    "sentence_window_retriever(\"SentenceWindow\", \"What's the agreement date?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0df26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sentence_window_rag(table_name, question):\n",
    "    ids, sentences = sentence_window_retriever(table_name, question, 5)\n",
    "    docs_str = \"\".join(sentences)\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the documents below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "\n",
    "<documents>\n",
    "{docs_str}\n",
    "</documents>\n",
    "<</SYS>>\n",
    "\n",
    "{question}[/INST]\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return prompt, cleaned_response\n",
    "\n",
    "data_augmented_prompt, answer = sentence_window_rag(\"SentenceWindow\", \"What's the agreement date?\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the CSV has 'Question' and 'Answer' columns\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_prompts = []\n",
    "generated_answers = []\n",
    "for question in questions:\n",
    "    data_augmented_prompt, generated_answer = sentence_window_rag(\"SentenceWindow\", question)\n",
    "    data_augmented_prompts.append(data_augmented_prompt)\n",
    "    generated_answers.append(generated_answer)\n",
    "\n",
    "generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd824ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_eval.metrics.generation.text import DeterministicAnswerCorrectness\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "evaluation_results = []\n",
    "for i in range(len(questions)):\n",
    "    ground_truth_embedding = embed_text(answers[i])\n",
    "    answer_embedding = embed_text(generated_answers[i])\n",
    "    cosine_similarity = util.cos_sim(ground_truth_embedding, answer_embedding)[0][0].item()\n",
    "    \n",
    "    datum = {\n",
    "        \"answer\": generated_answers[i],\n",
    "        \"ground_truth_answers\": [\n",
    "            answers[i]\n",
    "        ]\n",
    "    }\n",
    "    metric = DeterministicAnswerCorrectness()\n",
    "    eval_result = metric(**datum)\n",
    "    evaluation_results.append({\n",
    "        \"question\": data_augmented_prompts[i],\n",
    "        \"ref_answer\": answers[i],\n",
    "        \"response\": generated_answers[i],\n",
    "        \"semantic_similarity\": cosine_similarity,\n",
    "        \"token_overlap_recall\": eval_result[\"token_overlap_recall\"],\n",
    "        \"rouge_l_recall\": eval_result[\"rouge_l_recall\"]\n",
    "    })\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the file name\n",
    "csv_file = '../lab-data/sentence_window_rag_result.csv'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=evaluation_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(evaluation_results)\n",
    "\n",
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../lab-data/sentence_window_rag_result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "metrics = df.columns[-3:]\n",
    "\n",
    "# Prepare the data\n",
    "sentence_window_series_names = [f\"Sentence Window RAG {name}\" for name in metrics.tolist()]\n",
    "sentence_window_series_data = [df[metric].tolist() for metric in metrics]\n",
    "\n",
    "# Plot individual series bar charts\n",
    "for i in range(3):\n",
    "    plot_barchart(\n",
    "        [series_names[i], sentence_window_series_names[i]],\n",
    "        [series_data[i], sentence_window_series_data[i]],\n",
    "        list(range(len(series_data[i])))\n",
    "    )\n",
    "\n",
    "# Calculate the average for each metric\n",
    "sentence_window_average_scores = [sum(data) / len(data) for data in sentence_window_series_data]\n",
    "\n",
    "# Plot the average scores bar chart\n",
    "plot_barchart(['Naive RAG Average Scores', 'Sentence Window RAG Average Scores'], [average_scores, sentence_window_average_scores], metrics.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280f37c",
   "metadata": {},
   "source": [
    "#### Hypothetical Questions Optimization\n",
    "\n",
    "Purpose: Enhance retrieval relevance by transforming documents into hypothetical questions that are more likely to match user question semantically.\n",
    "\n",
    "Implementation: Use LLM to generate a few hypothetical questions from each chunk. Embed the questions in vector database for retrieval, and when rendering the final prompt, use the corresponding original chunks instead of the hypothetical questions as knowledge.\n",
    "\n",
    "Outcome: This approach increases the specificity and relevance of the retrieved content, improving the context provided to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "\n",
    "def generate(prompt):\n",
    "    # Create the request payload\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0,  # Adjust the randomness of the output\n",
    "        \"max_gen_len\": 128\n",
    "    }\n",
    "\n",
    "    # Initialize the Bedrock runtime client\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "    # Invoke the model\n",
    "    response = client.invoke_model(\n",
    "        modelId='meta.llama3-8b-instruct-v1:0',\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    byte_response = response['body'].read()\n",
    "    json_string = byte_response.decode('utf-8')\n",
    "\n",
    "    # Get the chat response\n",
    "    response_body = json.loads(json_string)\n",
    "    chat_response = response_body.get('generation')\n",
    "\n",
    "    return chat_response\n",
    "\n",
    "\n",
    "def generate_hypothetical_questions(question):\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Given the following text, generate a list of hypothetical questions that could be asked about the content, each in a single line. The questions should focus on key details, definitions, and information present in the text.\n",
    "<</SYS>>\n",
    "<text>\n",
    "{question}\n",
    "</text>\n",
    "<questions>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>', '', generate(prompt))\n",
    "    return [question for question in cleaned_response.split('\\n') if len(question) > 10]\n",
    "\n",
    "questions = generate_hypothetical_questions(\"\"\"Exhibit 10.24 \\n \\n[***]\\nCertain confidential information contained in this document, marked by brackets, has been omitted and filed separately with the\\nSecurities and Exchange Commission pursuant to Rule 24b-2 of the Securities Exchange Act of 1934, as amended.\\n \\nEXECUTION VERSION\\n \\nSTRATEGIC ALLIANCE AGREEMENT\\n \\nTHIS STRATEGIC ALLIANCE AGREEMENT (“Agreement”) is made and entered into as of November 6, 2016 (the “Effective Date”) by\\nand between Dialog Semiconductor (UK) Ltd., a corporation organized under the laws of England and Wales, having its principal office at 100\\nLongwater Avenue, Green Park, Reading, RG2 6GP, United Kingdom (“DIALOG”) and Energous Corporation, a Delaware corporation, having its\\nprincipal office at 3590 North First Street, Suite 210, San Jose, CA 95134 (“ENERGOUS”).\\n \\nWHEREAS DIALOG is a supplier of mixed-signal semiconductor products;\\n \\nWHEREAS ENERGOUS is a supplier of uncoupled wirefree charging systems, including antennas, semiconductors, firmware, software,\\nalgorithms, and sensors;\"\"\")\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunk text into smaller segments with a specified chunk size and overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be chunked.\n",
    "    - chunk_size (int): The size of each chunk.\n",
    "    - overlap (int): The number of characters that overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError(\"Chunk size must be greater than overlap\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "\n",
    "    while start < len(text):\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "        end = start + chunk_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(pdf_text_mupdf, 1024, 256)\n",
    "print(len(chunks))\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522aa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.drop_table(\"HypotheticalQuestions\")\n",
    "db.create_table(\n",
    "  table_name=\"HypotheticalQuestions\",\n",
    "  table_fields=[\n",
    "    {\"name\": \"Question\", \"dataType\": \"STRING\"},\n",
    "    {\"name\": \"ChunkID\", \"dataType\": \"INT\"},\n",
    "    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 512}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b77fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "records = []\n",
    "for index in tqdm(range(len(chunks)), desc=\"Processing chunks\"):\n",
    "    questions = generate_hypothetical_questions(chunks[index])\n",
    "    for question in questions:\n",
    "        records.append({\n",
    "            \"Question\": question,\n",
    "            \"ChunkID\": index,\n",
    "            \"Embedding\": embed_text(question)\n",
    "        })\n",
    "\n",
    "print(records[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert(\"HypotheticalQuestions\", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothetical_question_retriever(table_name, question, top_k):\n",
    "    code, resp = db.query(\n",
    "        table_name=table_name,\n",
    "        query_field=\"Embedding\",\n",
    "        query_vector=embed_text(question),\n",
    "        limit=top_k\n",
    "    )\n",
    "    chunk_list = []\n",
    "    for item in resp[\"result\"]:\n",
    "        if item[\"ChunkID\"] not in chunk_list:  # Check if the ID is already in the list\n",
    "            chunk_list.append(item[\"ChunkID\"])  # Append the ID if it's not a duplicate\n",
    "\n",
    "    return chunk_list, [chunks[id] for id in chunk_list]\n",
    "hypothetical_question_retriever(\"HypotheticalQuestions\", \"What's the agreement date?\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothetical_question_rag(table_name, question):\n",
    "    ids, docs = hypothetical_question_retriever(table_name, question, 15)\n",
    "    docs_str = \"------------------------\\n\"\n",
    "    for doc in docs:\n",
    "        docs_str += doc + \"\\n------------------------\\n\"\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the documents below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "Don't give answers to \n",
    "\n",
    "<documents>\n",
    "{docs_str}\n",
    "</documents>\n",
    "<</SYS>>\n",
    "\n",
    "{question}[/INST]\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return prompt, cleaned_response\n",
    "\n",
    "data_augmented_prompt, answer = hypothetical_question_rag(\"HypotheticalQuestions\", \"What's the agreement date?\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the CSV has 'Question' and 'Answer' columns\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_prompts = []\n",
    "generated_answers = []\n",
    "for index in tqdm(range(len(questions)), desc=\"Answering questions\"):\n",
    "    question = questions[index]\n",
    "    data_augmented_prompt, generated_answer = hypothetical_question_rag(\"HypotheticalQuestions\", question)\n",
    "    data_augmented_prompts.append(data_augmented_prompt)\n",
    "    generated_answers.append(generated_answer)\n",
    "\n",
    "generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09582ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "for i in range(len(questions)):\n",
    "    ground_truth_embedding = embed_text(answers[i])\n",
    "    answer_embedding = embed_text(generated_answers[i])\n",
    "    cosine_similarity = util.cos_sim(ground_truth_embedding, answer_embedding)[0][0].item()\n",
    "    \n",
    "    datum = {\n",
    "        \"answer\": generated_answers[i],\n",
    "        \"ground_truth_answers\": [\n",
    "            answers[i]\n",
    "        ]\n",
    "    }\n",
    "    metric = DeterministicAnswerCorrectness()\n",
    "    eval_result = metric(**datum)\n",
    "    evaluation_results.append({\n",
    "        \"question\": data_augmented_prompts[i],\n",
    "        \"ref_answer\": answers[i],\n",
    "        \"response\": generated_answers[i],\n",
    "        \"semantic_similarity\": cosine_similarity,\n",
    "        \"token_overlap_recall\": eval_result[\"token_overlap_recall\"],\n",
    "        \"rouge_l_recall\": eval_result[\"rouge_l_recall\"]\n",
    "    })\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f51b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the file name\n",
    "csv_file = '../lab-data/hypothetical_question_rag_result.csv'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=evaluation_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(evaluation_results)\n",
    "\n",
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../lab-data/hypothetical_question_rag_result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "metrics = df.columns[-3:]\n",
    "\n",
    "# Prepare the data\n",
    "hypothetical_question_series_names = [f\"Hypothetical Question RAG {name}\" for name in metrics.tolist()]\n",
    "hypothetical_question_series_data = [df[metric].tolist() for metric in metrics]\n",
    "\n",
    "# Plot individual series bar charts\n",
    "for i in range(3):\n",
    "    plot_barchart(\n",
    "        [series_names[i], hypothetical_question_series_names[i]],\n",
    "        [series_data[i], hypothetical_question_series_data[i]],\n",
    "        list(range(len(series_data[i])))\n",
    "    )\n",
    "\n",
    "# Calculate the average for each metric\n",
    "hypothetical_question_average_scores = [sum(data) / len(data) for data in hypothetical_question_series_data]\n",
    "\n",
    "# Plot the average scores bar chart\n",
    "plot_barchart(['Naive RAG Average Scores', 'Hypothetical Question RAG Average Scores'], [average_scores, hypothetical_question_average_scores], metrics.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447ae3a",
   "metadata": {},
   "source": [
    "#### Query Decomposition\n",
    "Purpose: Handle complex queries by breaking them down into simpler sub-queries, each targeting a specific aspect of the original question.\n",
    "\n",
    "Implementation: A complex query is decomposed into multiple, more manageable sub-queries. Each sub-query is processed independently through the retrieval system, and the results are combined before being passed to the language model.\n",
    "\n",
    "Outcome: This technique helps in better addressing multifaceted queries by ensuring that all relevant aspects are covered in the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23058ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_question(question):\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Given the following complex query, analyze whether it requires decomposition into simpler subqueries. If the query is already straightforward and can be answered directly, do not decompose it. If the query is complex or involves multiple steps, break it down into a sequence of subqueries that can be addressed individually. Each subquestion in a line.\n",
    "<</SYS>>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "[/INST]\n",
    "<sub_questions>\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return [question for question in cleaned_response.split('\\n') if len(question) > 10][:4]\n",
    "\n",
    "sub_questions = decompose_question(\"\"\"Does one party have the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law?\"\"\")\n",
    "\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.drop_table(\"NaiveRAG\")\n",
    "db.create_table(\n",
    "  table_name=\"NaiveRAG\",\n",
    "  table_fields=[\n",
    "    {\"name\": \"ID\", \"dataType\": \"INT\", \"primaryKey\": True},\n",
    "    {\"name\": \"Doc\", \"dataType\": \"STRING\"},\n",
    "    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 512}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436371a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\n",
    "        \"ID\": index,\n",
    "        \"Doc\": text,\n",
    "        \"Embedding\": embed_text(text)\n",
    "    }\n",
    "    for index, text in enumerate(chunks)\n",
    "]\n",
    "records[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0468eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert(\"NaiveRAG\", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_retriever(table_name, question, top_k):\n",
    "    code, resp = db.query(\n",
    "        table_name=table_name,\n",
    "        query_field=\"Embedding\",\n",
    "        query_vector=embed_text(question),\n",
    "        limit=top_k\n",
    "    )\n",
    "    return resp[\"result\"]\n",
    "basic_retriever(\"NaiveRAG\", \"What's the agreement date?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rag(table_name, question):\n",
    "    docs = basic_retriever(table_name, question, 5)\n",
    "    docs_str = \"------------------------\\n\"\n",
    "    for doc in docs:\n",
    "        docs_str += doc[\"Doc\"] + \"\\n------------------------\\n\"\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the documents below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "\n",
    "<documents>\n",
    "{docs_str}\n",
    "</documents>\n",
    "<</SYS>>\n",
    "[/INST]\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return prompt, cleaned_response\n",
    "\n",
    "data_augmented_prompt, answer = naive_rag(\"NaiveRAG\", \"What's the agreement date?\")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57601a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_question_resolver(sub_questions):\n",
    "    question_answer = \"\"\n",
    "    for sub_question in sub_questions:\n",
    "        data_augmented_prompt, answer = naive_rag(\"NaiveRAG\", sub_question)\n",
    "        question_answer += f\"Sub-Question: {sub_question}\\nAnswer: {answer}\\n------------------------\\n\"\n",
    "    return question_answer\n",
    "sub_question_resolver(decompose_question(\"Does one party have the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61012ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_query_rag(question):\n",
    "    sub_questions_qa = sub_question_resolver(decompose_question(question))\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the groud_truth section below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "\n",
    "<groud_truth>\n",
    "{sub_questions_qa}\n",
    "</groud_truth>\n",
    "<</SYS>>\n",
    "[/INST]\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "<answer>\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return prompt, cleaned_response\n",
    "\n",
    "data_augmented_prompt, answer = sub_query_rag(\"Does one party have the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law?\")\n",
    "\n",
    "data_augmented_prompt, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the CSV has 'Question' and 'Answer' columns\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_prompts = []\n",
    "generated_answers = []\n",
    "for index in tqdm(range(len(questions)), desc=\"Answering questions\"):\n",
    "    question = questions[index]\n",
    "    data_augmented_prompt, generated_answer = sub_query_rag(question)\n",
    "    data_augmented_prompts.append(data_augmented_prompt)\n",
    "    generated_answers.append(generated_answer)\n",
    "\n",
    "generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc938e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "for i in range(len(questions)):\n",
    "    ground_truth_embedding = embed_text(answers[i])\n",
    "    answer_embedding = embed_text(generated_answers[i])\n",
    "    cosine_similarity = util.cos_sim(ground_truth_embedding, answer_embedding)[0][0].item()\n",
    "    \n",
    "    datum = {\n",
    "        \"answer\": generated_answers[i],\n",
    "        \"ground_truth_answers\": [\n",
    "            answers[i]\n",
    "        ]\n",
    "    }\n",
    "    metric = DeterministicAnswerCorrectness()\n",
    "    eval_result = metric(**datum)\n",
    "    evaluation_results.append({\n",
    "        \"question\": data_augmented_prompts[i],\n",
    "        \"ref_answer\": answers[i],\n",
    "        \"response\": generated_answers[i],\n",
    "        \"semantic_similarity\": cosine_similarity,\n",
    "        \"token_overlap_recall\": eval_result[\"token_overlap_recall\"],\n",
    "        \"rouge_l_recall\": eval_result[\"rouge_l_recall\"]\n",
    "    })\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the file name\n",
    "csv_file = '../lab-data/query_decomposition_rag_result.csv'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=evaluation_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(evaluation_results)\n",
    "\n",
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../lab-data/query_decomposition_rag_result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "metrics = df.columns[-3:]\n",
    "\n",
    "# Prepare the data\n",
    "query_decomposition_series_names = [f\"Query Decomposition RAG {name}\" for name in metrics.tolist()]\n",
    "query_decomposition_series_data = [df[metric].tolist() for metric in metrics]\n",
    "\n",
    "# Plot individual series bar charts\n",
    "for i in range(3):\n",
    "    plot_barchart(\n",
    "        [series_names[i], query_decomposition_series_names[i]],\n",
    "        [series_data[i], query_decomposition_series_data[i]],\n",
    "        list(range(len(series_data[i])))\n",
    "    )\n",
    "\n",
    "# Calculate the average for each metric\n",
    "query_decomposition_average_scores = [sum(data) / len(data) for data in query_decomposition_series_data]\n",
    "\n",
    "# Plot the average scores bar chart\n",
    "plot_barchart(['Naive RAG Average Scores', 'Query Decomposition RAG Average Scores'], [average_scores, query_decomposition_average_scores], metrics.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0250f",
   "metadata": {},
   "source": [
    "#### Query Decomposition + Hypothetical Questions\n",
    "Purpose: Combine the strengths of both Hypothetical Question Generation and Query Decomposition for handling complex queries with targeted specificity.\n",
    "\n",
    "Implementation: The query is first decomposed into sub-queries, and each sub-query is then using the Hypothetical Question pipeline to get the answer. This hybrid approach aims to maximize the relevance and coverage of the retrieved content.\n",
    "\n",
    "Outcome: This method provides the most comprehensive and contextually enriched responses, particularly for complex queries, by ensuring both broad coverage and targeted retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_question_resolver_hypothetical_question(sub_questions):\n",
    "    question_answer = \"\"\n",
    "    for sub_question in sub_questions:\n",
    "        data_augmented_prompt, answer = hypothetical_question_rag(\"HypotheticalQuestions\", sub_question)\n",
    "        question_answer += f\"Sub-Question: {sub_question}\\nAnswer: {answer}\\n------------------------\\n\"\n",
    "    return question_answer\n",
    "sub_question_resolver_hypothetical_question(decompose_question(\"Does one party have the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_query_rag_with_hypothetical_question(question):\n",
    "    ids, docs = hypothetical_question_retriever(\"HypotheticalQuestions\", question, 15)\n",
    "    docs_str = \"------------------------\\n\"\n",
    "    for doc in docs:\n",
    "        docs_str += doc + \"\\n------------------------\\n\"\n",
    "    sub_questions_qa = sub_question_resolver_hypothetical_question(decompose_question(question))\n",
    "    prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Your answer should be grounded by the information provided in the documents and groud_truth section below.\n",
    "Don't make up answers.\n",
    "Don't explain your thought process.\n",
    "Directly answer the question in concise way.\n",
    "\n",
    "<documents>\n",
    "{docs_str}\n",
    "</documents>\n",
    "\n",
    "<groud_truth>\n",
    "{sub_questions_qa}\n",
    "</groud_truth>\n",
    "<</SYS>>\n",
    "[/INST]\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "<answer>\n",
    "\"\"\"\n",
    "    cleaned_response = re.sub(r'</?[^>]+>|\\[/?(INST|SYS)\\]', '', generate(prompt))\n",
    "    return prompt, cleaned_response\n",
    "\n",
    "data_augmented_prompt, answer = sub_query_rag_with_hypothetical_question(\"What's the agreement date?\")\n",
    "\n",
    "data_augmented_prompt, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d82aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = '../lab-data/ENERGOUSCORP_qa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the CSV has 'Question' and 'Answer' columns\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_prompts = []\n",
    "generated_answers = []\n",
    "for index in tqdm(range(len(questions)), desc=\"Answering questions\"):\n",
    "    question = questions[index]\n",
    "    data_augmented_prompt, generated_answer = sub_query_rag_with_hypothetical_question(question)\n",
    "    data_augmented_prompts.append(data_augmented_prompt)\n",
    "    generated_answers.append(generated_answer)\n",
    "\n",
    "generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b99988",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "for i in range(len(questions)):\n",
    "    ground_truth_embedding = embed_text(answers[i])\n",
    "    answer_embedding = embed_text(generated_answers[i])\n",
    "    cosine_similarity = util.cos_sim(ground_truth_embedding, answer_embedding)[0][0].item()\n",
    "    \n",
    "    datum = {\n",
    "        \"answer\": generated_answers[i],\n",
    "        \"ground_truth_answers\": [\n",
    "            answers[i]\n",
    "        ]\n",
    "    }\n",
    "    metric = DeterministicAnswerCorrectness()\n",
    "    eval_result = metric(**datum)\n",
    "    evaluation_results.append({\n",
    "        \"question\": data_augmented_prompts[i],\n",
    "        \"ref_answer\": answers[i],\n",
    "        \"response\": generated_answers[i],\n",
    "        \"semantic_similarity\": cosine_similarity,\n",
    "        \"token_overlap_recall\": eval_result[\"token_overlap_recall\"],\n",
    "        \"rouge_l_recall\": eval_result[\"rouge_l_recall\"]\n",
    "    })\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa45f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name\n",
    "csv_file = '../lab-data/advanced_rag_result.csv'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=evaluation_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(evaluation_results)\n",
    "\n",
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../lab-data/advanced_rag_result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "metrics = df.columns[-3:]\n",
    "\n",
    "# Prepare the data\n",
    "qd_and_hq_series_names = [f\"Query Decomposition + Hypothetical Question RAG {name}\" for name in metrics.tolist()]\n",
    "qd_and_hq_series_data = [df[metric].tolist() for metric in metrics]\n",
    "\n",
    "# Plot individual series bar charts\n",
    "for i in range(3):\n",
    "    plot_barchart(\n",
    "        [series_names[i], qd_and_hq_series_names[i]],\n",
    "        [series_data[i], qd_and_hq_series_data[i]],\n",
    "        list(range(len(series_data[i])))\n",
    "    )\n",
    "\n",
    "# Calculate the average for each metric\n",
    "qd_and_hq_average_scores = [sum(data) / len(data) for data in qd_and_hq_series_data]\n",
    "\n",
    "# Plot the average scores bar chart\n",
    "plot_barchart(['Naive RAG Average Scores', 'Query Decomposition + Hypothetical Question RAG Average Scores'], [average_scores, qd_and_hq_average_scores], metrics.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb88dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
