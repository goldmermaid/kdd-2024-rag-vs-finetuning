<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="2487.7">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 28.0px Times; -webkit-text-stroke: #000000}
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px '.SF NS'; color: #000000; -webkit-text-stroke: rgba(255, 255, 255, 0.5); background-color: #ffffff; min-height: 17.0px}
    p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px '.SF NS'; color: #000000; -webkit-text-stroke: #6556e4}
    p.p4 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px Times; color: #262626; -webkit-text-stroke: #262626}
    p.p5 {margin: 0.0px 0.0px 0.0px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000; min-height: 29.0px}
    p.p6 {margin: 0.0px 0.0px 0.0px 0.0px; font: 24.0px Times; -webkit-text-stroke: #000000}
    p.p7 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000; min-height: 14.0px}
    p.p8 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px Times; -webkit-text-stroke: #000000}
    p.p9 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000; min-height: 18.0px}
    p.p10 {margin: 0.0px 0.0px 12.0px 0.0px; font: 12.0px Times; -webkit-text-stroke: #000000; min-height: 14.0px}
    p.p11 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px Times; -webkit-text-stroke: #000000; min-height: 23.0px}
    p.p12 {margin: 0.0px 0.0px 12.0px 0.0px; font: 14.0px Times; -webkit-text-stroke: #000000; min-height: 18.0px}
    span.s1 {font-kerning: none}
    span.s2 {font: 18.0px Times; font-kerning: none; color: #000000; -webkit-text-stroke: 0px #000000}
    span.s3 {font: 18.0px '.SF NS'; font-kerning: none; background-color: #ffffff; -webkit-text-stroke: 0px #6556e4}
    span.s4 {font-kerning: none; color: #000000; -webkit-text-stroke: 0px #000000}
    span.s5 {font-kerning: none; background-color: #ffffff}
    span.s6 {font: 12.0px Times; font-kerning: none}
  </style>
</head>
<body>
<p class="p1"><span class="s1"><b>Domain-Driven LLM Development: Insights into RAG and Fine- Tuning Practices</b></span></p>
<p class="p2"><span class="s1"></span><br></p>
<p class="p3"><span class="s2">Hands On Tutorials at<span class="Apple-converted-space">  </span><a href="https://www.ecai2024.eu/"><span class="s3">2024 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Barcelona, Spain</span></a></span></p>
<p class="p4"><span class="s4">Schedule: </span><span class="s5">14:00 – 17:00, August 25, 2024</span></p>
<p class="p5"><span class="s1"><b></b></span><br></p>
<p class="p6"><span class="s1"><b>Abstract</b></span></p>
<p class="p7"><span class="s1"></span><br></p>
<p class="p8"><span class="s1">To improve Large Language Model (LLM) performance on domain specific applications, ML developers often leverage Retrieval Augmented Generation (RAG) and LLM Fine-Tuning. RAG extends the capabilities of LLMs to specific domains or an organization's internal knowledge base, without the need to retrain the model. On the other hand, Fine-Tuning approach updates LLM weights with domain-specific data to improve performance on specific tasks. The fine-tuned model is particularly effective to systematically learn new comprehensive knowledge in a specific domain that is not covered by the LLM pre-training. This tutorial walks through the RAG and Fine-Tuning techniques, discusses the insights of their advantages and limitations, and provides best practices of adopting the methodologies for the LLM tasks anduse cases. The hands-on labs demonstrate the advanced techniques to optimize the RAG and fine-tuned LLM architecture that handles domain specific LLM tasks. The labs in the tutorial are designed by using a set of open-source python libraries to implement the RAG and fine-tuned LLM architecture.</span></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p6"><span class="s1"><b>Agenda</b></span></p>
<p class="p7"><span class="s1"></span><br></p>
<p class="p8"><span class="s1"><b>Section 1: Introduction</b> to RAG and LLM Fine-Tuning (20 mins) <b><span class="Apple-converted-space"> </span></b></span></p>
<p class="p8"><span class="s1"><b>Section 2: </b>Lab setup (10 mins)</span></p>
<p class="p8"><span class="s1"><b>Section 3: Lab 1:</b> Advanced Techniques in RAG (40 mins) - Richard Song</span></p>
<p class="p8"><span class="s1"><b>Break</b> (10 mins)</span></p>
<p class="p8"><span class="s1"><b>Section 4: Lab 2:</b> LLM Fine-Tuning (40 mins) - Yunfei Bai, Rachel Hu<span class="Apple-converted-space"> </span></span></p>
<p class="p8"><span class="s1"><b>Break</b> (10 mins)</span></p>
<p class="p8"><span class="s1"><b>Section 5: Lab 3:</b> RAG and Fine-Tuned Model Benchmarking (30 mins) - José Cassio dos Santos Junior<span class="Apple-converted-space"> </span></span></p>
<p class="p8"><span class="s1"><b>Section 6: Conclusion</b> and Q&amp;A<b> </b>(20 mins)<span class="Apple-converted-space"> </span></span></p>
<p class="p10"><span class="s1"></span><br></p>
<p class="p6"><span class="s1"><b>Presenters</b></span><span class="s6"><span class="Apple-converted-space"> </span></span></p>
<p class="p7"><span class="s1"></span><br></p>
<p class="p8"><span class="s1"><b>José Cassio dos Santos Junior<span class="Apple-converted-space"> </span></b></span></p>
<p class="p8"><span class="s1">A member of the Amazon Machine Learning University team. He is responsible for Curriculum Development for Advanced Modules. As a previous Senior Data Scientist on the AWS LATAM Professional Services Data Science team, he was responsible for experiments and MVP engagements with important clients in the Brazilian market. Cassio has over 20 years of experience working as a software engineer. As a business process management expert, he participated in BPO projects for more than 7 years. Cass also has more than 10 years of teaching experience at colleges and acting as instructor for Linux certification preparation and Microsoft Innovation Center bootcamps. He holds a Master’s degree in Computer Engineering, a Bachelor’s degree in Physics, and a Bachelor’s degree in Business Administration, specialized in IT Quantitative Methods.</span></p>
<p class="p11"><span class="s1"><b></b></span><br></p>
<p class="p8"><span class="s1"><b>Rachel Hu<span class="Apple-converted-space"> </span></b></span></p>
<p class="p8"><span class="s1">The Co-founder &amp; CEO of CambioML, which builds open-source libraries to prepare high quality data for RAG and LLM finetuning. Previously she was an Applied Scientist at AWS AI, an ML instructor at Amazon Machine Learning University, and a speaker at top conferences including KDD, NVIDIA GTC, AWS re:Invent, MLOps Summit, etc. Rachel co-authored Dive into Deep Learning (D2L.ai), an open-source interactive textbook adopted by over 500 universities around the world. Rachel received her master degree of statistics from University of California, Berkeley, and bachelor of Math from University of Waterloo, Canada.</span></p>
<p class="p11"><span class="s1"><b></b></span><br></p>
<p class="p8"><span class="s1"><b>Richard Song<span class="Apple-converted-space"> </span></b></span></p>
<p class="p8"><span class="s1">The Co-Founder and CEO of Epsilla Inc, a one-stop RAGaaS platform for building production ready LLM applications. With a background in big data, vector graph databases, and high performance computing, Richard helps customers build production-ready RAG systems connected with large scale proprietary data. Richard holds a Master’s degree in Computer Science from Cornell University.</span></p>
<p class="p11"><span class="s1"><b></b></span><br></p>
<p class="p8"><span class="s1"><b>Yunfei Bai<span class="Apple-converted-space"> </span></b></span></p>
<p class="p8"><span class="s1">A Senior Solutions Architect at Amazon Web Services. With over 15 years’ experience on AI/ML, Data Science and Analytics, Yunfei helps AWS customers adopt AI/ML and Generative AI services to deliver business results. Prior to AWS, he worked in various roles including product manager and solution consultant in multiple industries, designed and delivered AI/ML and data analytics solutions that overcome complex technical challenges and drive strategic objectives. Yunfei has a PhD in Electronic and Electrical Engineering. He has published research papers and blog posts, and serves as a journal reviewer.</span></p>
<p class="p12"><span class="s1"></span><br></p>
<p class="p6"><span class="s1"><b>Materials</b></span></p>
<p class="p9"><span class="s1"></span><br></p>
<p class="p8"><span class="s1"><b>Slides</b>: coming soon</span></p>
<p class="p8"><span class="s1"><b>Github</b>: coming soon<span class="Apple-converted-space"> </span></span></p>
<p class="p11"><span class="s1"></span><br></p>
<p class="p11"><span class="s1"></span><br></p>
<p class="p9"><span class="s1"></span><br></p>
</body>
</html>
